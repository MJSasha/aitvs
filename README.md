Компиляция:

```shell
gcc lab1.cpp -I"C:\Program Files (x86)\Microsoft SDKs\MPI\Include" -L"C:\Program Files (x86)\Microsoft SDKs\MPI\Lib\x64" -lmsmpi -o lab1.exe
```

Запуск

```shell
mpiexec -n 4 lab1.exe
```

# Пример 1. Рассылка массива между MPI-процессами

>В процессе с максимальным номером в группе формируется произвольный массив (например, массив вещественных чисел длины 100000). Затем этот массив рассылается всем процессам группы, так что каждый процесс получает копию данного массива. При выполнении демонстрируется применение функции MPI_Bcast.

```c
#include <stdio.h>
#include <stdlib.h>
#include <mpi.h>

#define ARRAY_SIZE 100000

int main(int argc, char *argv[]) {
    int rank, numprocs;
    double *array = NULL; // указатель на массив, который будет рассылаться

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);   // Получаем номер текущего процесса
    MPI_Comm_size(MPI_COMM_WORLD, &numprocs); // Получаем общее количество процессов

    /* Процесс с максимальным номером формирует исходный массив */
    if (rank == numprocs - 1) {
        array = (double *)malloc(ARRAY_SIZE * sizeof(double));
        for (int i = 0; i < ARRAY_SIZE; i++) {
            array[i] = (double)i * 0.5;  // Заполняем произвольными значениями
        }
        printf("Процесс %d подготовил массив данных длины %d\n", rank, ARRAY_SIZE);
    } else {
        /* Остальные процессы готовят память для приёма массива */
        array = (double *)malloc(ARRAY_SIZE * sizeof(double));
    }

    /* Рассылка массива: все процессы получают копию массива
       Используем MPI_Bcast, вызываемую всеми процессами.
       Корневой процесс выбирается с номером, равным numprocs-1 */
    MPI_Bcast(array, ARRAY_SIZE, MPI_DOUBLE, numprocs - 1, MPI_COMM_WORLD);

    /* Каждый процесс может проверить, что данные получены корректно: */
    printf("Процесс %d получил элемент array[0] = %f\n", rank, array[0]);

    free(array);
    MPI_Finalize();
    return 0;
}
```

**Результаты:**

![imj](./img/lab1.png)

**Пояснения:**  
1. Функции `MPI_Comm_rank` и `MPI_Comm_size` используются для определения номера процесса и общего числа процессов в группе.  
2. Процесс с максимальным номером (номер numprocs–1) формирует исходный массив.  
3. Функция `MPI_Bcast` рассылает данные (в данном случае массив типа `double`) от корневого процесса всем остальным.  
4. Замер времени (например, с помощью `MPI_Wtime`) может быть дополнительно добавлен для анализа производительности рассылки.

---

# Пример 2. Параллельное вычисление суммы с использованием MPI_Reduce

>В каждом процессе создаётся переменная типа `double`, значение которой определяется как 0.01 * номер процесса. Задача состоит в том, чтобы найти сумму этих переменных с использованием процедуры MPI_Reduce. Результат (сумма) передаётся в процесс с номером 0. Также демонстрируется применение функций MPI_Comm_rank, MPI_Comm_size.

```c
#include <stdio.h>
#include <mpi.h>

int main(int argc, char *argv[]) {
    int rank, numprocs;
    double a, sum; // a - локальное значение, sum - результат редукции

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);   // Получаем номер текущего процесса
    MPI_Comm_size(MPI_COMM_WORLD, &numprocs); // Получаем общее количество процессов

    /* Каждый процесс вычисляет своё значение */
    a = 0.01 * rank;
    printf("Процесс %d имеет значение a = %f\n", rank, a);

    /* Выполняется операция редукции (суммирование) */
    MPI_Reduce(&a, &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

    /* Процесс с номером 0 выводит результат */
    if (rank == 0) {
        printf("Сумма значений от всех процессов = %f\n", sum);
    }

    MPI_Finalize();
    return 0;
}
```

**Результаты:**

![imj](./img/lab2.png)

**Пояснения:**  
1. Использование функций `MPI_Comm_rank` и `MPI_Comm_size` обеспечивает корректное распределение вычислений среди процессов.  
2. Каждый процесс вычисляет значение `a = 0.01 * rank`.  
3. Операция редукции `MPI_Reduce` с операцией `MPI_SUM` собирает сумму всех переменных `a` в переменную `sum`, которая находится у процесса с номером 0.  
4. Дополнительно можно реализовать параллельное выполнение цикла (например, если имеется массив или более объёмный набор данных), распределив работу между процессами с помощью разбиения итераций (например, блочного или циклического), как указано в учебном пособии (см. пример с умножением элементов массива на константу).

---


# 3. Пример распределения цикла (умножение элементов массива на константу):

Представим, что имеется массив `A` длины `N`, над которым необходимо выполнить умножение каждого элемента на 10. Для распределения вычислений между `np` MPI-процессами можно воспользоваться следующей стратегией:

1. **Блочное разбиение:**  
   Каждый процесс получает непрерывный блок итераций.  
   Например, для процесса с номером `rank` диапазон итераций:  
   \[
   start = \frac{N}{np} \times rank,\quad end = start + \frac{N}{np}
   \]
2. **Циклическое разбиение:**  
   Каждый процесс обрабатывает элементы с индексами, удовлетворяющими условию  
   \[
   i \mod np = rank
   \]

Такой подход позволяет параллельно выполнять цикл, минимизируя накладные расходы на коммуникацию.

```c
#include <stdio.h>
#include <stdlib.h>
#include <mpi.h>

int main(int argc, char *argv[]) {
    int rank, numprocs;
    int N = 1000000; // длина массива
    double *A = NULL;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);

    // Выделяем память для массива (лишь у процесса 0, далее данные рассылаются)
    if (rank == 0) {
        A = (double *)malloc(N * sizeof(double));
        for (int i = 0; i < N; i++) {
            A[i] = i;
        }
    }

    // Определяем размеры блоков для каждого процесса
    int chunk = N / numprocs;
    double *local_A = (double *)malloc(chunk * sizeof(double));

    // Рассылка блока данных каждому процессу (используем MPI_Scatter)
    MPI_Scatter(A, chunk, MPI_DOUBLE, local_A, chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);

    // Каждый процесс преобразует свой блок: умножает элементы на 10
    for (int i = 0; i < chunk; i++) {
        local_A[i] = local_A[i] * 10.0;
    }

    // Сбор обработанных блоков в исходный массив на процессе 0 (MPI_Gather)
    MPI_Gather(local_A, chunk, MPI_DOUBLE, A, chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);

    // Процесс 0 может проверить результат
    if (rank == 0) {
        printf("После умножения A[0] = %f, A[N-1] = %f\n", A[0], A[N-1]);
        free(A);
    }
    free(local_A);

    MPI_Finalize();
    return 0;
}
```

**Результаты:**

![imj](./img/lab3.png)

**Пояснения:**  
- Здесь для демонстрации применяется коммуникация MPI через функции `MPI_Scatter` и `MPI_Gather`.  
- Распределение работы происходит путём деления исходного массива на непрерывные блоки (блочный принцип).
- Каждый процесс работает параллельно над своим блоком, что иллюстрирует параллельное исполнение цикла.
